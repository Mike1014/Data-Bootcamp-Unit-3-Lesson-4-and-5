{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1.Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "\n",
    "    This is a regression problem because we are targeting the 'running times' which is a continous variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.You have more features (columns) than rows in your dataset.\n",
    "\n",
    "    Personally, I dont like the idea of having more columns than rows, so I want to reduce the features with feature selection first. Using the filter, wrapper, or embedded methods. There is also the option of SVM (RBF).\n",
    "    \n",
    "    Note: PCA will not work because not enough data because less rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3.Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "\n",
    "    We can make this into a binary classification problem; jailed before 20 = '0' and jailed after 20 = '1'. And then use logistic regression to find the coefficients for each features. Higher coefficient in absolute value would generally means more important feature.\n",
    "    \n",
    "    Note: Random Forest, Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4.Implement a filter to “highlight” emails that might be important to the recipient\n",
    "\n",
    "    This is a Naives Bayes problem because we need to go through all the emails and identify key terms. This is simply the opposite of 'spam' filter. Instead of looking for key words to put a 'spam' label on the emails, we highlight the emails for being important. I would use Bag of Words and Term Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5.You have 1000+ features.\n",
    "\n",
    "    Need to reduce the number of features because there are just too many. Feature selection, PCA, or SVM kernel tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6.Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "\n",
    "    I think Naives Bayes, Random Forest, and KNN can all work well for this\n",
    "    \n",
    "    (mention a few example of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 7.Your dataset dimensions are 982400 x 500\n",
    "\n",
    "    Use sampling, can also use dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 8.Identify faces in an image.\n",
    "\n",
    "    Create a classifer to determine if this is a face or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 9.Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "\n",
    "    (Start off simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
